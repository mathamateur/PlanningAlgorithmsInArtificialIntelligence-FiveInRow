{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "narrative-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from typing import Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recorded-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gomoku:\n",
    "    \n",
    "    '''\n",
    "    Gym-like environmet for Chienese game Gomoku (a.k.a. Five-In-Row)\n",
    "    '''\n",
    "    def __init__(self, height=10, width=10, len_to_win=5, current_player=1):\n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.len_to_win = len_to_win\n",
    "        \n",
    "        # Board for game.\n",
    "        # Possible values 0, 1, 2\n",
    "        # 0 - empty field\n",
    "        # 1 - chip of 1st player\n",
    "        # 2 - chip of 2nd player\n",
    "        self.state = np.zeros((height, width))\n",
    "        \n",
    "        # Player's turn\n",
    "        # 1 - turn of 1st player\n",
    "        # 2 - turn of 2nd player\n",
    "        self.current_player = current_player\n",
    "        \n",
    "    def _check_winner(self):\n",
    "        \n",
    "        # Exclude first and last self.len_to_win // 2 rows and columns.\n",
    "        # Will check it later\n",
    "        for i in range(self.len_to_win // 2, self.height - self.len_to_win // 2):\n",
    "            for j in range(self.len_to_win // 2, self.width - self.len_to_win // 2):\n",
    "                \n",
    "                # Check diagonals\n",
    "                main_d, side_d = 0, 0\n",
    "                \n",
    "                for k in range(-(self.len_to_win // 2), self.len_to_win // 2 + 1):\n",
    "                    \n",
    "                    \n",
    "                    if self.state[i + k][j + k] == self.current_player:\n",
    "                        \n",
    "                        main_d += 1\n",
    "                        \n",
    "                    if self.state[i - k][j + k] == self.current_player:\n",
    "                        \n",
    "                        side_d += 1\n",
    "                        \n",
    "                # Check, is there a line with length self.len_to_win\n",
    "                \n",
    "                if self.len_to_win in (main_d, side_d,\n",
    "                                       np.sum(self.state[i - self.len_to_win: i + self.len_to_win + 1] == self.current_player),\n",
    "                                       np.sum(self.state[j - self.len_to_win: j + self.len_to_win + 1] == self.current_player)):\n",
    "                    return True, self.current_player\n",
    "                \n",
    "            # Check first and last self.len_to_win rows\n",
    "            for i in range(self.len_to_win // 2):\n",
    "                \n",
    "                for j in range(self.width - self.len_to_win + 1):\n",
    "                    \n",
    "                    if self.len_to_win in (\n",
    "                        np.sum(self.state[i, j:j + self.len_to_win + 1] == self.current_player),\n",
    "                        np.sum(self.state[self.height - i - 1, j:j + self.len_to_win + 1] == self.current_player)\n",
    "                    ):\n",
    "                        \n",
    "                        return True, self.current_player\n",
    "                    \n",
    "            # Check first and last self.len_to_win columns\n",
    "            for j in range(self.len_to_win // 2):\n",
    "                \n",
    "                for i in range(self.height - self.len_to_win + 1):\n",
    "                    \n",
    "                    if self.len_to_win in (\n",
    "                        np.sum(self.state[i:i + self.len_to_win + 1, j] == self.current_player),\n",
    "                        np.sum(self.state[i:i + self.len_to_win + 1, self.width - j - 1] == self.current_player)\n",
    "                    ):\n",
    "                        \n",
    "                        return True, self.current_player\n",
    "                    \n",
    "        return False, -1\n",
    "                    \n",
    "    \n",
    "    def _get_reward(self):\n",
    "        \n",
    "        '''Return rewards both for the 1st and for 2nd player'''\n",
    "        \n",
    "        flag, player = self._check_winner()\n",
    "        \n",
    "        if flag:\n",
    "            \n",
    "            rewards = [-1, -1]\n",
    "            \n",
    "            rewards[self.current_player - 1] = 1\n",
    "            \n",
    "            return rewards, True\n",
    "        \n",
    "        return [0, 0], False\n",
    "    \n",
    "    def available_actions(self):\n",
    "        \n",
    "        # Rows and columns\n",
    "        return list(zip(*np.where(self.state == 0)))\n",
    "        \n",
    "    def reset(self, current_player=1):\n",
    "        \n",
    "        '''Start the new game from initial position'''\n",
    "        \n",
    "        self.state = np.zeros((self.height, self.width))\n",
    "        self.current_player = current_player\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action: Tuple):\n",
    "        \n",
    "        # Action is a tuple with (i, j) coordinates, \n",
    "        # where the current player place his chip\n",
    "        \n",
    "        self.state[action] = self.current_player\n",
    "        rewards, is_done = self._get_reward()\n",
    "        number_of_available_actions = self.height * self.width - len(self.available_actions())\n",
    "        \n",
    "        self.current_player = 2 - self.current_player + 1\n",
    "        \n",
    "        if is_done or number_of_available_actions == 0:\n",
    "            \n",
    "            # Terminal state\n",
    "            return self.state, rewards, True\n",
    "        \n",
    "        return self.state, rewards, False\n",
    "        \n",
    "    def render(self):\n",
    "        \n",
    "        print(*self.state, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "unlimited-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_value_fn(env):\n",
    "    '''\n",
    "    Calculate the longest line for each player which \n",
    "    doesn't contain chips the opponent.\n",
    "    '''\n",
    "    longest_line = [0, 0]\n",
    "    \n",
    "    for player in range(1, 3):\n",
    "        \n",
    "        # Check main and side diaogonals\n",
    "        \n",
    "        for i in range(env.len_to_win // 2, env.height - env.len_to_win // 2):\n",
    "            for j in range(env.len_to_win // 2, env.width - env.len_to_win // 2):\n",
    "                \n",
    "                main_d, side_d = 0, 0\n",
    "                \n",
    "                for k in range(-(env.len_to_win // 2), env.len_to_win // 2 + 1):\n",
    "                    \n",
    "                    if env.state[i + k][j + k] == player:\n",
    "                        \n",
    "                        main_d += 1\n",
    "                        \n",
    "                    elif env.state[i + k][j + k] == 2 - player + 1:\n",
    "                        \n",
    "                        main_d = -10\n",
    "                        \n",
    "                    if env.state[i - k][j + k] == player:\n",
    "                        \n",
    "                        side_d += 1\n",
    "                        \n",
    "                    elif env.state[i - k][j + k] == 2 - player + 1:\n",
    "                        \n",
    "                        side_d = -10\n",
    "                        \n",
    "                longest_line[player - 1] = max([longest_line[player - 1], main_d, side_d])\n",
    "                   \n",
    "        # Check rows\n",
    " \n",
    "        for i in range(env.height):\n",
    "            for j in range(env.width - env.len_to_win + 1):\n",
    "                \n",
    "                row = 0 \n",
    "                for k in range(env.len_to_win - 1):\n",
    "                    \n",
    "                    if env.state[i][j + k] == player:\n",
    "\n",
    "                        row += 1\n",
    "\n",
    "                    elif env.state[i][j + k] == 2 - player + 1:\n",
    "\n",
    "                        row = -10\n",
    "                        \n",
    "                longest_line[player - 1] = max(longest_line[player - 1], row)\n",
    "                \n",
    "        # Check columns\n",
    "        \n",
    "        for i in range(env.height - env.len_to_win + 1):\n",
    "            for j in range(env.width):\n",
    "                \n",
    "                column = 0\n",
    "                for k in range(env.len_to_win - 1):\n",
    "                    \n",
    "                    if env.state[i + k][j] == player:\n",
    "\n",
    "                        column += 1\n",
    "\n",
    "                    elif env.state[i + k][j] == 2 - player + 1:\n",
    "\n",
    "                        column = -10\n",
    "                    \n",
    "                longest_line[player - 1] = max(longest_line[player - 1], column)\n",
    "                \n",
    "    max_player_proxy, min_player_proxy = longest_line\n",
    "\n",
    "    if max_player_proxy == 5:\n",
    "        return 1\n",
    "    elif min_player_proxy == 5:\n",
    "        return -1\n",
    "\n",
    "    # convert score to [-1, 1] range (actually to smaller range, to make actual win more valuable)\n",
    "    return (max_player_proxy - min_player_proxy) / env.len_to_win\n",
    "        \n",
    "    \n",
    "    \n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, env, depth=0, is_terminal=False, heuristic_value_fn=heuristic_value_fn):\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.env = env\n",
    "        self.is_terminal = is_terminal\n",
    "        self.heuristic_value_fn = heuristic_value_fn\n",
    "    \n",
    "    def get_heuristic_value(self):\n",
    "        \n",
    "        return self.heuristic_value_fn(self.env)\n",
    "\n",
    "\n",
    "def _ndarray_to_tuple(ndarray):\n",
    "\n",
    "    return tuple(map(tuple, ndarray))\n",
    "\n",
    "    \n",
    "def alphabeta(env, node, depth, alpha, beta, maximizingPlayer, visited_states):\n",
    "    #print(f\"Hi, depth {depth}\")\n",
    "    if depth == 0:\n",
    "        return node.get_heuristic_value()\n",
    "    \n",
    "    if maximizingPlayer:\n",
    "        \n",
    "        value = -float('inf')\n",
    "        \n",
    "        for action in env.available_actions():\n",
    "            #print(action)\n",
    "            env_copy = deepcopy(env)\n",
    "            \n",
    "            new_state, reward, is_terminal = env_copy.step(action)\n",
    "            #print(env_copy.state)\n",
    "            #print(is_terminal)\n",
    "            #print(reward)\n",
    "            \n",
    "            new_state_tuple = _ndarray_to_tuple(new_state)\n",
    "            \n",
    "            if new_state_tuple not in visited_states:\n",
    "                \n",
    "                visited_states.add(new_state_tuple)\n",
    "                \n",
    "                new_node = Node(env_copy, depth - 1)\n",
    "            \n",
    "                if is_terminal:\n",
    "\n",
    "                    node.is_terminal = True\n",
    "\n",
    "                    return reward[0] #node.get_heuristic_value(maximizingPlayer)\n",
    "                \n",
    "                score = alphabeta(env_copy, new_node, depth - 1, alpha, beta, False, visited_states)\n",
    "                #print(score, value)\n",
    "                value = max(value, score)\n",
    "            \n",
    "            if value >= beta:\n",
    "                \n",
    "                break\n",
    "                \n",
    "            alpha = max(alpha, value)\n",
    "            \n",
    "        return value\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        value = float('inf')\n",
    "        \n",
    "        for action in env.available_actions():\n",
    "            #print(action)\n",
    "            \n",
    "            env_copy = deepcopy(env)\n",
    "            #print(env_copy.state)\n",
    "            new_state, reward, is_terminal = env_copy.step(action)\n",
    "            #print(reward)\n",
    "            \n",
    "            new_state_tuple = _ndarray_to_tuple(new_state)\n",
    "            \n",
    "            if new_state_tuple not in visited_states:\n",
    "                \n",
    "                visited_states.add(new_state_tuple)\n",
    "                \n",
    "                new_node = Node(env_copy, depth - 1)\n",
    "            \n",
    "                if is_terminal:\n",
    "\n",
    "                    node.is_terminal = True\n",
    "\n",
    "                    return -reward[1] #node.get_heuristic_value(maximizingPlayer)\n",
    "                \n",
    "                value = min(value, alphabeta(env_copy, new_node, depth - 1, alpha, beta, True, visited_states))\n",
    "            \n",
    "            if value <= alpha:\n",
    "                \n",
    "                break\n",
    "                \n",
    "            beta = min(beta, value)\n",
    "            \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8e2e558f-56f3-4a69-98d5-67b09755473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "env = Gomoku(height=3, width=3, len_to_win=3)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2d4aa3a3-6917-4061-b3da-18d2a9d64b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0], False)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state = np.array([\n",
    "    [2, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0]\n",
    "])\n",
    "env.current_player = 1\n",
    "\n",
    "env._get_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e277c179-b8b8-4381-8966-f80377383b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3333333333333333"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heuristic_value_fn(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a399f3f1-57e2-4296-9c0a-779cd0439dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Node(env)\n",
    "visited_states = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1e90b1d5-23b5-4cca-839a-43265409dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = alphabeta(env, root, 5, -np.inf, np.inf, maximizingPlayer=True, visited_states=visited_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3dba047d-1c92-4b78-b7e7-1323928b8e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3333333333333333"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308fea17-6111-4e23-b861-6ed6d11d6b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f074c-5648-4d2e-a5b1-096eb4b8ee32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05382f-bb33-405c-98cf-cd3ff5b073fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "conservative-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(actions):\n",
    "    \n",
    "    return np.random.choice(actions)\n",
    "\n",
    "class MCTS:\n",
    "    \n",
    "    def __init__(self, default_policy=random_policy, c_ucb=5, n_simulations=100):\n",
    "        \n",
    "        self.default_policy = default_policy\n",
    "        self.c_ucb = c_ucb\n",
    "        self.n_simulations = n_simulations\n",
    "        \n",
    "    def _ndarray_to_tuple(self, ndarray):\n",
    "        \n",
    "        return tuple(map(tuple, ndarray))\n",
    "        \n",
    "    def selection(self, env, root: Tuple, tree: Tuple, visited_states: Dict):\n",
    "    \n",
    "        q = deque()\n",
    "        q.append(root)\n",
    "\n",
    "        while q:\n",
    "\n",
    "            state = q.pop()\n",
    "\n",
    "            candidate_states = []\n",
    "\n",
    "            for a in env.available_actions():\n",
    "                \n",
    "                new_env = deepcopy(env)\n",
    "                \n",
    "                new_state, reward, flag = new_env.step(a)\n",
    "                \n",
    "                new_state = self._ndarray_to_tuple(new_state)\n",
    "\n",
    "                if new_state not in tree:\n",
    "\n",
    "                    visited_states.update({new_state: state})\n",
    "\n",
    "                    return new_state, visited_states\n",
    "\n",
    "                if new_state not in visited_states:\n",
    "\n",
    "                    candidate_states.append(new_state)\n",
    "\n",
    "            if len(candidate_states) > 0:\n",
    "\n",
    "                best_state = max(candidate_states, key=lambda s: tree[s])\n",
    "\n",
    "                q.appendleft(best_state)\n",
    "                visited_states.update({best_state: state})\n",
    "                \n",
    "\n",
    "        return root, visited_states\n",
    "\n",
    "    def game(env, state_e: Tuple, state_p: Tuple, default_policy: np.ndarray, max_steps=50):\n",
    "\n",
    "        trajectory = [state_e]\n",
    "\n",
    "        s_e = state_e\n",
    "        s_p = state_p\n",
    "\n",
    "        for i in range(max_steps):\n",
    "\n",
    "            u = action_space[default_policy[s_e]]\n",
    "\n",
    "            new_state_e, _ = transition_function(env, s_e, u)\n",
    "\n",
    "            new_state_p = pursuer_transition(env, s_e, s_p)\n",
    "\n",
    "            trajectory.append(new_state_e)\n",
    "\n",
    "            s_e = new_state_e\n",
    "\n",
    "            s_p = new_state_p\n",
    "\n",
    "            if new_state_e in (goal, new_state_p):\n",
    "\n",
    "                break\n",
    "\n",
    "        return trajectory, s_p\n",
    "\n",
    "\n",
    "    def simulation(env, state_e: Tuple, state_p: Tuple, goal: Tuple, default_policy: np.ndarray, n_iters: int = 50):\n",
    "\n",
    "        rewards = []\n",
    "\n",
    "        for i in range(n_iters):\n",
    "\n",
    "            trajectory, s_p = game(env, state_e, state_p, default_policy)\n",
    "\n",
    "            reward = get_reward(trajectory, s_p, goal)\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "        mean_reward = np.mean(rewards)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "\n",
    "    def backpropagation(tree, new_state, visited_states, mean_reward):\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        while state is not None:\n",
    "\n",
    "            tree[state] = max(tree[state], mean_reward)\n",
    "\n",
    "            state = visited_states[state]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "romantic-dutch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "env = Gomoku(height=3, width=3, len_to_win=3)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intelligent-hybrid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1.0, 2.0, 1.0), (1.0, 2.0, 1.0), (0.0, 0.0, 2.0)),\n",
       " {((1.0, 2.0, 1.0),\n",
       "   (1.0, 2.0, 1.0),\n",
       "   (0.0, 0.0, 2.0)): array([[1., 2., 0.],\n",
       "         [1., 2., 1.],\n",
       "         [0., 0., 2.]])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts = MCTS()\n",
    "env.step((0, 0))\n",
    "env.step((0, 1))\n",
    "env.step((1, 0))\n",
    "env.step((1, 1))\n",
    "env.step((1, 2))\n",
    "env.step((2, 2))\n",
    "mcts.selection(env, env.state, {}, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "personalized-vinyl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 0.]\n",
      "[1. 2. 1.]\n",
      "[0. 0. 2.]\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bizarre-ballet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "willing-dancing",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "selection() missing 3 required positional arguments: 'root', 'tree', and 'visited_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-630afb96cf62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmcts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: selection() missing 3 required positional arguments: 'root', 'tree', and 'visited_states'"
     ]
    }
   ],
   "source": [
    "mcts = MCTS()\n",
    "mcts.selection(env, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Play:\n",
    "    \n",
    "    def __init__(self, env, player1, player2, n_episods):\n",
    "        \n",
    "        self.env = env\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        \n",
    "        self.n_episods = n_episods\n",
    "        \n",
    "        self.rewards = []\n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        for i in tqdm(range(self.n_episods)):\n",
    "            \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
