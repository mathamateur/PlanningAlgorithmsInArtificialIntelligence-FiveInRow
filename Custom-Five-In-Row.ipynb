{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "temporal-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "agricultural-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gomoku:\n",
    "    \n",
    "    '''\n",
    "    Gym-like environmet for Chienese game Gomoku (a.k.a. Five-In-Row)\n",
    "    '''\n",
    "    def __init__(self, height=10, width=10, len_to_win=5, current_player=1):\n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.len_to_win = len_to_win\n",
    "        \n",
    "        # Board for game.\n",
    "        # Possible values 0, 1, 2\n",
    "        # 0 - empty field\n",
    "        # 1 - chip of 1st player\n",
    "        # 2 - chip of 2nd player\n",
    "        self.state = np.zeros((height, width))\n",
    "        \n",
    "        # Player's turn\n",
    "        # 1 - turn of 1st player\n",
    "        # 2 - turn of 2nd player\n",
    "        self.current_player = current_player\n",
    "        \n",
    "    def _check_winner(self):\n",
    "        \n",
    "        # Exclude first and last self.len_to_win // 2 rows and columns.\n",
    "        # Will check it later\n",
    "        for i in range(self.len_to_win // 2, self.height - self.len_to_win // 2):\n",
    "            for j in range(self.len_to_win // 2, self.width - self.len_to_win // 2):\n",
    "                \n",
    "                # Check diagonals\n",
    "                main_d, side_d = 0, 0\n",
    "                \n",
    "                for k in range(-(self.len_to_win // 2), self.len_to_win // 2 + 1):\n",
    "                    \n",
    "                    \n",
    "                    if self.state[i + k][j + k] == self.current_player:\n",
    "                        \n",
    "                        main_d += 1\n",
    "                        \n",
    "                    if self.state[i - k][j + k] == self.current_player:\n",
    "                        \n",
    "                        side_d += 1\n",
    "                        \n",
    "                # Check, is there a line with length self.len_to_win\n",
    "                \n",
    "                if self.len_to_win in (main_d, side_d,\n",
    "                                       np.sum(self.state[i - self.len_to_win: i + self.len_to_win + 1] == self.current_player),\n",
    "                                       np.sum(self.state[j - self.len_to_win: j + self.len_to_win + 1] == self.current_player)):\n",
    "                    return True, self.current_player\n",
    "                \n",
    "            # Check first and last self.len_to_win rows\n",
    "            for i in range(self.len_to_win // 2):\n",
    "                \n",
    "                for j in range(self.width - self.len_to_win + 1):\n",
    "                    \n",
    "                    if self.len_to_win in (\n",
    "                        np.sum(self.state[i, j:j + self.len_to_win + 1]),\n",
    "                        np.sum(self.state[self.height - i - 1, j:j + self.len_to_win + 1])\n",
    "                    ):\n",
    "                        \n",
    "                        return True, self.current_player\n",
    "                    \n",
    "            # Check first and last self.len_to_win columns\n",
    "            for j in range(self.len_to_win // 2):\n",
    "                \n",
    "                for i in range(self.height - self.len_to_win + 1):\n",
    "                    \n",
    "                    if self.len_to_win in (\n",
    "                        np.sum(self.state[i:i + self.len_to_win + 1, j]),\n",
    "                        np.sum(self.state[i:i + self.len_to_win + 1, self.width - j - 1])\n",
    "                    ):\n",
    "                        \n",
    "                        return True, self.current_player\n",
    "                    \n",
    "        return False, -1\n",
    "                    \n",
    "    \n",
    "    def _get_reward(self):\n",
    "        \n",
    "        '''Return rewards both for the 1st and for 2nd player'''\n",
    "        \n",
    "        flag, player = self._check_winner()\n",
    "        \n",
    "        if flag:\n",
    "            \n",
    "            rewards = [-1, -1]\n",
    "            \n",
    "            rewards[self.current_player - 1] = 1\n",
    "            \n",
    "            return rewards, True\n",
    "        \n",
    "        return [0, 0], False\n",
    "    \n",
    "    def available_actions(self):\n",
    "        \n",
    "        # Rows and columns\n",
    "        return list(zip(*np.where(self.state == 0)))\n",
    "        \n",
    "    def reset(self, current_player=1):\n",
    "        \n",
    "        '''Start the new game from initial position'''\n",
    "        \n",
    "        self.state = np.zeros((self.height, self.width))\n",
    "        self.current_player = current_player\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action: Tuple):\n",
    "        \n",
    "        # Action is a tuple with (i, j) coordinates, \n",
    "        # where the current player place his chip\n",
    "        \n",
    "        self.state[action] = self.current_player\n",
    "        rewards, is_done = self._get_reward()\n",
    "        number_of_available_actions = self.height * self.width - len(self.available_actions())\n",
    "        \n",
    "        self.current_player = 2 - self.current_player + 1\n",
    "        \n",
    "        if is_done or number_of_available_actions == 0:\n",
    "            \n",
    "            # Terminal state\n",
    "            return self.state, rewards, True\n",
    "        \n",
    "        return self.state, rewards, False\n",
    "        \n",
    "    def render(self):\n",
    "        \n",
    "        print(*self.state, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "prescription-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_value_fn(env):\n",
    "    '''\n",
    "    Calculate the longest line for each player which \n",
    "    doesn't contain chips the opponent.\n",
    "    '''\n",
    "    longest_line = [0, 0]\n",
    "    \n",
    "    for player in range(1, 3):\n",
    "        \n",
    "        # Check main and side diaogonals\n",
    "        \n",
    "        for i in range(env.len_to_win // 2, env.height - env.len_to_win // 2):\n",
    "            for j in range(env.len_to_win // 2, env.width - env.len_to_win // 2):\n",
    "                \n",
    "                main_d, side_d = 0, 0\n",
    "                \n",
    "                for k in range(-(env.len_to_win // 2), env.len_to_win // 2 + 1):\n",
    "                    \n",
    "                    if env.state[i + k][j + k] == player:\n",
    "                        \n",
    "                        main_d += 1\n",
    "                        \n",
    "                    elif env.state[i + k][j + k] == 2 - player + 1:\n",
    "                        \n",
    "                        main_d = -10\n",
    "                        \n",
    "                    if env.state[i - k][j + k] == player:\n",
    "                        \n",
    "                        side_d += 1\n",
    "                        \n",
    "                    elif env.state[i - k][j + k] == 2 - player + 1:\n",
    "                        \n",
    "                        side_d = -10\n",
    "                        \n",
    "                longest_line[player - 1] = max([longest_line[player - 1], main_d, side_d])\n",
    "                   \n",
    "        # Check rows\n",
    " \n",
    "        for i in range(env.height):\n",
    "            for j in range(env.width - env.len_to_win + 1):\n",
    "                \n",
    "                row = 0 \n",
    "                for k in range(env.len_to_win - 1):\n",
    "                    \n",
    "                    if env.state[i][j + k] == player:\n",
    "\n",
    "                        row += 1\n",
    "\n",
    "                    elif env.state[i][j + k] == 2 - player + 1:\n",
    "\n",
    "                        row = -10\n",
    "                        \n",
    "                longest_line[player - 1] = max(longest_line[player - 1], row)\n",
    "                \n",
    "        # Check columns\n",
    "        \n",
    "        for i in range(env.height - env.len_to_win + 1):\n",
    "            for j in range(env.width):\n",
    "                \n",
    "                column = 0\n",
    "                for k in range(env.len_to_win - 1):\n",
    "                    \n",
    "                    if env.state[i + k][j] == player:\n",
    "\n",
    "                        column += 1\n",
    "\n",
    "                    elif env.state[i + k][j] == 2 - player + 1:\n",
    "\n",
    "                        column = -10\n",
    "                    \n",
    "                longest_line[player - 1] = max(longest_line[player - 1], column)\n",
    "                \n",
    "    return longest_line\n",
    "    \n",
    "    \n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, state, depth=0, is_terminal=False, heuristic_value_fn=heuristic_value_fn):\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.state = state\n",
    "        self.is_terminal = is_terminal\n",
    "    \n",
    "    def get_heuristic_value(self):\n",
    "        \n",
    "        return self.heuristic_value_fn(self.state)\n",
    "    \n",
    "def alphabeta(env, node, parent, depth, alpha, beta, maximazingPlayer, visited_states):\n",
    "    \n",
    "    if depth == 0:\n",
    "        \n",
    "        return node.get_heuristic_value()\n",
    "    \n",
    "    if maximazingPlayer:\n",
    "        \n",
    "        value = -float('inf')\n",
    "        \n",
    "        for action in env.available_actions():\n",
    "            \n",
    "            env_copy = deepcopy(env)\n",
    "            \n",
    "            new_state, reward, is_terminal = env_copy.step(action)\n",
    "            \n",
    "            new_state = tuple(new_state)\n",
    "            \n",
    "            if new_state not in visited_states:\n",
    "                \n",
    "                visited_states.add(new_state)\n",
    "                \n",
    "                new_node = Node(new_state, depth - 1)\n",
    "            \n",
    "                if is_terminal:\n",
    "\n",
    "                    node.is_terminal = True\n",
    "\n",
    "                    return node.get_heuristic_value()\n",
    "                \n",
    "                value = np.max(value, alphabeta(new_node, node, depth - 1, alpha, beta, False, visited_states))\n",
    "            \n",
    "            if value >= beta:\n",
    "                \n",
    "                break\n",
    "                \n",
    "            alpha = np.max(alpha, value)\n",
    "            \n",
    "        return value\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        value = float('inf')\n",
    "        \n",
    "        for action in env.available_actions():\n",
    "            \n",
    "            env_copy = deepcopy(env)\n",
    "            \n",
    "            new_state, reward, is_terminal = env_copy.step(action)\n",
    "            \n",
    "            new_state = tuple(new_state)\n",
    "            \n",
    "            if new_state not in visited_states:\n",
    "                \n",
    "                visited_states.add(new_state)\n",
    "                \n",
    "                new_node = Node(new_state, depth - 1)\n",
    "            \n",
    "                if is_terminal:\n",
    "\n",
    "                    node.is_terminal = True\n",
    "\n",
    "                    return node.get_heuristic_value()\n",
    "                \n",
    "                value = np.min(value, alphabeta(new_node, node, depth - 1, alpha, beta, True, visited_states))\n",
    "            \n",
    "            if value <= alpha:\n",
    "                \n",
    "                break\n",
    "                \n",
    "            beta = np.min(beta, value)\n",
    "            \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(actions):\n",
    "    \n",
    "    return np.random.choice(actions)\n",
    "\n",
    "class MCTS:\n",
    "    \n",
    "    def __init__(self, default_policy, c_ucb=5, n_simulations=100):\n",
    "        \n",
    "        self.default_policy = default_policy\n",
    "        self.c_ucb = c_ucb\n",
    "        self.n_simulations = n_simulations\n",
    "        \n",
    "        \n",
    "    def selection(self, env, root, tree, visited_states):\n",
    "    \n",
    "        q = deque()\n",
    "        q.append(root)\n",
    "\n",
    "        while q:\n",
    "\n",
    "            state = q.pop()\n",
    "\n",
    "            candidate_states = []\n",
    "\n",
    "            for a in env.available_actions():\n",
    "                \n",
    "                new_env = deepcopy(env)\n",
    "\n",
    "                new_state, reward, flag = env.step(a)\n",
    "                \n",
    "                new_state = tuple(new_state)\n",
    "\n",
    "                if new_state not in tree:\n",
    "\n",
    "                    visited_states.update({new_state: state})\n",
    "\n",
    "                    return new_state, visited_states\n",
    "\n",
    "                if new_state not in visited_states:\n",
    "\n",
    "                    candidate_states.append(new_state)\n",
    "\n",
    "            if len(candidate_states) > 0:\n",
    "\n",
    "\n",
    "                best_state = max(candidate_states, key=lambda s: tree[s])\n",
    "\n",
    "                q.appendleft(best_state)\n",
    "                visited_states.update({best_state: state})\n",
    "\n",
    "        return root, visited_states\n",
    "            \n",
    "\n",
    "    def game(env, state_e: Tuple, state_p: Tuple, default_policy: np.ndarray, max_steps=50):\n",
    "\n",
    "        trajectory = [state_e]\n",
    "\n",
    "        s_e = state_e\n",
    "        s_p = state_p\n",
    "\n",
    "        for i in range(max_steps):\n",
    "\n",
    "            u = action_space[default_policy[s_e]]\n",
    "\n",
    "            new_state_e, _ = transition_function(env, s_e, u)\n",
    "\n",
    "            new_state_p = pursuer_transition(env, s_e, s_p)\n",
    "\n",
    "            trajectory.append(new_state_e)\n",
    "\n",
    "            s_e = new_state_e\n",
    "\n",
    "            s_p = new_state_p\n",
    "\n",
    "            if new_state_e in (goal, new_state_p):\n",
    "\n",
    "                break\n",
    "\n",
    "        return trajectory, s_p\n",
    "\n",
    "\n",
    "    def simulation(env, state_e: Tuple, state_p: Tuple, goal: Tuple, default_policy: np.ndarray, n_iters: int = 50):\n",
    "\n",
    "        rewards = []\n",
    "\n",
    "        for i in range(n_iters):\n",
    "\n",
    "            trajectory, s_p = game(env, state_e, state_p, default_policy)\n",
    "\n",
    "            reward = get_reward(trajectory, s_p, goal)\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "        mean_reward = np.mean(rewards)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "\n",
    "    def backpropagation(tree, new_state, visited_states, mean_reward):\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        while state is not None:\n",
    "\n",
    "            tree[state] = max(tree[state], mean_reward)\n",
    "\n",
    "            state = visited_states[state]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "stock-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Gomoku()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "dirty-tamil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heuristic_value_fn(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "processed-democrat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step((0, 0))\n",
    "env.step((9, 9))\n",
    "env.step((0, 5))\n",
    "env.step((8, 8))\n",
    "env.step((9, 0))\n",
    "env.step((7, 7))\n",
    "env.step((3, 3))\n",
    "env.step((5, 5))\n",
    "heuristic_value_fn(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "developed-airplane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 2. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 2. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 2. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 2.]\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "checked-beach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Play:\n",
    "    \n",
    "    def __init__(self, env, player1, player2, n_episods):\n",
    "        \n",
    "        self.env = env\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        \n",
    "        self.n_episods = n_episods\n",
    "        \n",
    "        self.rewards = []\n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        for i in tqdm(range(self.n_episods)):\n",
    "            \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
